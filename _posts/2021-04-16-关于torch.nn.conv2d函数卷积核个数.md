---
layout:     post
title:      关于nn.conv2d()的卷积核个数
subtitle:   Torch中nn.conv2d()函数操作过程
date:       2021-04-16
author:     Zeekinger
header-img: img/post-bg-seu.jpg
catalog: true
tags:
    - Torch
    - Convolution 
    - Deep Learning 

---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>


> 在使用torch的卷积函数nn.Conv2d()时，发现其形参中并没有提供卷积核个数的传值参数，官方文档中亦没有对卷 积过程进行相关说明。不同的卷积过程得到的特征图是不同的，同时对整个网络的性能也有不同的影响，这一点在卷积神经网络的开山之作《Gradient-Based Learning Applied to Document Recognition》中Ⅱ.B 部分有相关解释。本文在查阅相关资料后，对torch中的
nn.Conv2d（）函数的卷积过程进行简单归纳。 根据输入通道的多少，将卷积分为单通道卷积和多通道卷积。

## 卷积函数的输入特征图为单通道时
这种情况较为简单，也比较好理解。 此时卷积核数等于输出通道数out_channels，即用out_channels个卷积核分别对输入特征进行卷积操作，可以学习到out_channels种特征。
## 卷积函数的输入通道为多通道时
不妨设相应的函数为nn.conv2d(in_channels=6,out_channels=16,kernel_size=3)。此时卷积操作由out_channels个的深度为in_channels大小为kernel_size的卷积核完成，即16个3×3×6的卷积核完成。可以等价为用16组卷积核分别对输入特征图进行卷积，学习到16个高维特征图。一组有6个3×3的卷积核，分别与输入特征图的6组通道进行卷积，然后进行pixel-wise求和操作，得到一个新的高维特征图。
## 卷积核的生成
在传统图像处理领域，每个filter的设计都具有特殊的目的，如用Prewitt算子、Sobel对图像进行去噪、增强边缘等。而卷积神经网络中的filter都是通过学习得到的，即每次nn.Conv2d()的卷积核都是随机生成的，torch中fillter内参数默认按uniform分布生成。

参考资料： https://zhuanlan.zhihu.com/p/32190799